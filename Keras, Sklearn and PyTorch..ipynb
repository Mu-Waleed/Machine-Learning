{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c20c180",
      "metadata": {
        "id": "3c20c180"
      },
      "source": [
        "# Machine Learning (CS535): Assignment 3\n",
        "## Neural Networks\n",
        "#### Name: Muhammad Waleed\n",
        "#### Roll Number: 22030017"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac26b33",
      "metadata": {
        "id": "bac26b33"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "\n",
        "*   The aim of this assignment is to learn machine learning tools - Keras, Sklearn and PyTorch.\n",
        "*   You must use the Python programming language.\n",
        "*   You can add as many code/markdown cells as required.\n",
        "*   ALL cells must be run (and outputs visible) in order to get credit for your work.\n",
        "*   Please use procedural programming style and comment your code thoroughly.\n",
        "*   There are three parts of this assignment. The import statemnets for the required libraries is already given.\n",
        "*   **Carefully read the submission instructions and plagiarism policy.**\n",
        "*   Deadline to submit this assignment is 17th November 2022, 11:55pm on LMS.\n",
        "*   TAs will not be allowed to debug your code or answer how to solve a question.\n",
        "\n",
        "### Submission Instructions\n",
        "\n",
        "You should submit both your notebook file (.ipynb), python script (.py) and dataset (.csv) on LMS.\n",
        "Please name your files Name_RollNo_Assignment3. Zip these files in a folder and name\n",
        "the folder Name_RollNo_Assignment3. If you don't know how to save .ipynb as .py see\n",
        "[this](https://i.stack.imgur.com/L1rQH.png). Failing to submit any one of them might result in the reduction of marks.\n",
        "\n",
        "### Plagiarism Policy\n",
        "\n",
        "The code $\\color{red}{\\text{MUST}}$ be done independently. Any plagiarism or cheating of work from others\n",
        "or the internet will be immediately referred to the DC. If you are confused about what\n",
        "constitutes plagiarism, it is your responsibility to consult with the instructor or the TA\n",
        "in a timely manner. **PLEASE DO NOT LOOK AT ANYONE ELSE'S CODE\n",
        "NOR DISCUSS IT WITH THEM.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "857325dc",
      "metadata": {
        "id": "857325dc"
      },
      "source": [
        "### Introduction\n",
        "In this assignment, you will be implementing neural network for the provided dataset using Sklearn, Keras and PyTorch. A description of the problem statement is given at the start of each part. \n",
        "\n",
        "Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6461902f",
      "metadata": {
        "id": "6461902f"
      },
      "outputs": [],
      "source": [
        "# Importing the modules\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ad1548",
      "metadata": {
        "id": "47ad1548"
      },
      "source": [
        "## Part 1: Feature Extraction\n",
        "You are given [MNIST audio dataset](https://drive.google.com/file/d/1imgBIVbgtGPV31r64UghSK3161Qfumsm/view?usp=sharing) which contains audio recordings, where speakers say digits (0 to 9) out loud. Use the following line of code to read the audio file:\n",
        "```python\n",
        "audio, sr = librosa.load(file_path, sr=16000)\n",
        "```\n",
        "You need to extract MFCC features for each audio file, the feature extraction code is give (you can read about MFCC from [here](https://link.springer.com/content/pdf/bbm:978-3-319-49220-9/1.pdf)). Length of each feature vector will be 13. You need to save all the feature vectors in a csv file with ith column representing ith feature, and each row representing an audio file. Add a 'y' column to the csv file and append the labels column at the end. Your csv file should look like this:\n",
        "\n",
        "| x1 | x2 | x3 | x4 | x5 | x6 | x7 | x8 | x9 | x10 | x11 | x12 | x13 | y |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| -11.347038 | -8.070062 | -0.915299 | 6.859546 | 8.754656 | -3.440287 | -5.738487 | -21.853178 | -9.859462 | 3.584948 | -2.661195\t| 1.023747 | -4.574332 | 2 |\n",
        "\n",
        "Split the dataset into train and test with 80:20 ratio. Print the train data size and test data size."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the python_speech_features\n",
        "!pip install python_speech_features"
      ],
      "metadata": {
        "id": "yj0-SyqbQ1zE"
      },
      "id": "yj0-SyqbQ1zE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "877edb4d",
      "metadata": {
        "id": "877edb4d"
      },
      "outputs": [],
      "source": [
        "# Importing the modules\n",
        "from glob import glob\n",
        "import python_speech_features as mfcc\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fef48f7",
      "metadata": {
        "id": "0fef48f7"
      },
      "outputs": [],
      "source": [
        "def get_MFCC(audio, sr):\n",
        "    features = mfcc.mfcc(audio, sr, 0.025, 0.01, 13, appendEnergy = True)\n",
        "    return np.mean(features, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UXkxz1rUnS5",
        "outputId": "ef73fb4a-5b94-4f6b-b8df-62dc177a2f70"
      },
      "id": "3UXkxz1rUnS5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b141033",
      "metadata": {
        "id": "3b141033"
      },
      "outputs": [],
      "source": [
        "# Iterating over the dataset and creating cvs file according to the requirements\n",
        "file_path = \"/content/drive/MyDrive/ML/output/data\"\n",
        "file_name, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, y = [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
        "\n",
        "\n",
        "for each_folder in os.listdir(file_path):\n",
        "  if not each_folder.endswith(\".txt\"):\n",
        "    sub_path = file_path + \"/\" + each_folder\n",
        "    for each_voice in os.listdir(sub_path):\n",
        "      audio, sr = librosa.load(sub_path + \"/\" + each_voice, sr=16000)\n",
        "      m = get_MFCC(audio, sr)\n",
        "      file_name.append(each_voice)\n",
        "      y.append(int(each_voice.split(\"_\")[0])), x1.append(float(m[0])), x2.append(float(m[1])), x3.append(float(m[2])), x4.append(float(m[3])), \n",
        "      x5.append(float(m[4])), x6.append(float(m[5])), x7.append(float(m[6])), x8.append(float(m[7])), x9.append(float(m[8])),\n",
        "      x10.append(float(m[9])), x11.append(float(m[10])), x12.append(float(m[11])), x13.append(float(m[12]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the dataframe to csv\n",
        "data = {'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4, 'x5': x5, 'x6': x6, 'x7': x7, 'x8': x8, 'x9': x9, 'x10': x10, 'x11': x11, 'x12': x12, 'x13': x13, \"y\": y} \n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('out.csv')"
      ],
      "metadata": {
        "id": "S7H-SwrzB-UD"
      },
      "id": "S7H-SwrzB-UD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading from the csv file\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ML/out.csv\")\n",
        "df = df.drop(df.columns[[0]], axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8EHbAnbObxaR",
        "outputId": "f4329a41-163a-4d30-efff-54888ff04925"
      },
      "id": "8EHbAnbObxaR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          File         x1        x2        x3         x4        x5        x6  \\\n",
              "0   0_01_0.wav -10.679614 -1.126155 -2.921500   7.777644 -4.234647 -1.907632   \n",
              "1   0_01_1.wav -11.423947 -1.890179 -1.148646  10.610124  1.231129 -4.728646   \n",
              "2  0_01_10.wav -10.253554 -2.093445 -5.647451   6.346362 -1.961829 -1.761370   \n",
              "3  0_01_11.wav -10.208797 -3.407698 -8.520919   3.171731 -1.332095 -7.890483   \n",
              "4  0_01_12.wav -10.512413 -4.236322 -0.662954   3.462385 -4.762044 -3.514585   \n",
              "\n",
              "          x7        x8        x9        x10       x11       x12        x13  y  \n",
              "0 -15.338491 -3.180810  2.313479 -10.125013 -3.455760  4.896240 -12.021362  0  \n",
              "1 -13.272780  1.202890  0.686237  -5.113064  2.272280  6.805703  -8.340184  0  \n",
              "2 -14.366297 -6.844078 -0.555402  -5.644106 -2.729775  4.582464  -7.005110  0  \n",
              "3 -19.013466 -1.750540  8.268122  -3.398884 -2.734833  8.183276 -12.306567  0  \n",
              "4 -16.976979 -0.667198  1.336080  -7.423439 -2.892023  5.200130  -8.368142  0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-740bdacf-df3c-4839-bce7-355966a1d1e5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>x10</th>\n",
              "      <th>x11</th>\n",
              "      <th>x12</th>\n",
              "      <th>x13</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0_01_0.wav</td>\n",
              "      <td>-10.679614</td>\n",
              "      <td>-1.126155</td>\n",
              "      <td>-2.921500</td>\n",
              "      <td>7.777644</td>\n",
              "      <td>-4.234647</td>\n",
              "      <td>-1.907632</td>\n",
              "      <td>-15.338491</td>\n",
              "      <td>-3.180810</td>\n",
              "      <td>2.313479</td>\n",
              "      <td>-10.125013</td>\n",
              "      <td>-3.455760</td>\n",
              "      <td>4.896240</td>\n",
              "      <td>-12.021362</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0_01_1.wav</td>\n",
              "      <td>-11.423947</td>\n",
              "      <td>-1.890179</td>\n",
              "      <td>-1.148646</td>\n",
              "      <td>10.610124</td>\n",
              "      <td>1.231129</td>\n",
              "      <td>-4.728646</td>\n",
              "      <td>-13.272780</td>\n",
              "      <td>1.202890</td>\n",
              "      <td>0.686237</td>\n",
              "      <td>-5.113064</td>\n",
              "      <td>2.272280</td>\n",
              "      <td>6.805703</td>\n",
              "      <td>-8.340184</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0_01_10.wav</td>\n",
              "      <td>-10.253554</td>\n",
              "      <td>-2.093445</td>\n",
              "      <td>-5.647451</td>\n",
              "      <td>6.346362</td>\n",
              "      <td>-1.961829</td>\n",
              "      <td>-1.761370</td>\n",
              "      <td>-14.366297</td>\n",
              "      <td>-6.844078</td>\n",
              "      <td>-0.555402</td>\n",
              "      <td>-5.644106</td>\n",
              "      <td>-2.729775</td>\n",
              "      <td>4.582464</td>\n",
              "      <td>-7.005110</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0_01_11.wav</td>\n",
              "      <td>-10.208797</td>\n",
              "      <td>-3.407698</td>\n",
              "      <td>-8.520919</td>\n",
              "      <td>3.171731</td>\n",
              "      <td>-1.332095</td>\n",
              "      <td>-7.890483</td>\n",
              "      <td>-19.013466</td>\n",
              "      <td>-1.750540</td>\n",
              "      <td>8.268122</td>\n",
              "      <td>-3.398884</td>\n",
              "      <td>-2.734833</td>\n",
              "      <td>8.183276</td>\n",
              "      <td>-12.306567</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0_01_12.wav</td>\n",
              "      <td>-10.512413</td>\n",
              "      <td>-4.236322</td>\n",
              "      <td>-0.662954</td>\n",
              "      <td>3.462385</td>\n",
              "      <td>-4.762044</td>\n",
              "      <td>-3.514585</td>\n",
              "      <td>-16.976979</td>\n",
              "      <td>-0.667198</td>\n",
              "      <td>1.336080</td>\n",
              "      <td>-7.423439</td>\n",
              "      <td>-2.892023</td>\n",
              "      <td>5.200130</td>\n",
              "      <td>-8.368142</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-740bdacf-df3c-4839-bce7-355966a1d1e5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-740bdacf-df3c-4839-bce7-355966a1d1e5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-740bdacf-df3c-4839-bce7-355966a1d1e5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training and testing\n",
        "x = df.iloc[:, 1:14]\n",
        "y = df.iloc[:, -1]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "print(\"x_train:\", x_train.shape)\n",
        "print(\"x_test:\", x_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcfbdmJMpTDA",
        "outputId": "df3f77da-c6b3-41f5-f610-c16bdf7193bd"
      },
      "id": "rcfbdmJMpTDA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train: (24000, 13)\n",
            "x_test: (6000, 13)\n",
            "y_train: (24000,)\n",
            "y_test (6000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "124f0ccd",
      "metadata": {
        "id": "124f0ccd"
      },
      "source": [
        "## Part 2: Neural Network Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1734848",
      "metadata": {
        "id": "b1734848"
      },
      "source": [
        "### Task 2.1:  Scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95541b60",
      "metadata": {
        "id": "95541b60"
      },
      "source": [
        "In this part you will use the [Scikit-learn](https://scikit-learn.org/stable/index.html) to implement the [Neural Network](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) and apply it to the MNIST audio dataset (provided in part 1). Split the training dataset into train and evaluation data with 90:10 ratio. Run evaluation on X_eval while training on X_train. Tune the hyperparameters to get the best possible classification accuracy. You need to report accuracy, recall, precision and F1 score on the test dataset and print the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa7f20d",
      "metadata": {
        "id": "aaa7f20d"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary modules\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e8e09b7",
      "metadata": {
        "id": "3e8e09b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072addce-161e-49dc-d67d-0ffd3042345e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train: (27000, 13)\n",
            "x_test: (3000, 13)\n",
            "y_train: (3000,)\n",
            "y_test (27000,)\n"
          ]
        }
      ],
      "source": [
        "# Splitting the dataset to train and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "\n",
        "print(\"x_train:\", x_train.shape)\n",
        "print(\"x_test:\", x_test.shape)\n",
        "print(\"y_train:\", y_test.shape)\n",
        "print(\"y_test\", y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the dataset\n",
        "clf = MLPClassifier(random_state=1, max_iter=300).fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "FqCLZqlGyQzR"
      },
      "id": "FqCLZqlGyQzR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on test dataset\n",
        "pred = clf.predict(x_test)\n",
        "print(classification_report(y_test, pred))\n",
        "print(confusion_matrix(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS1x9eVTynr-",
        "outputId": "22f24909-8547-4d0a-f1d6-16b02b3bccc3"
      },
      "id": "LS1x9eVTynr-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89       295\n",
            "           1       0.95      0.87      0.91       311\n",
            "           2       0.86      0.94      0.90       326\n",
            "           3       0.94      0.89      0.92       287\n",
            "           4       0.94      0.98      0.96       272\n",
            "           5       0.98      0.98      0.98       303\n",
            "           6       1.00      0.99      0.99       336\n",
            "           7       0.96      0.96      0.96       314\n",
            "           8       0.95      0.95      0.95       275\n",
            "           9       0.93      0.93      0.93       281\n",
            "\n",
            "    accuracy                           0.94      3000\n",
            "   macro avg       0.94      0.94      0.94      3000\n",
            "weighted avg       0.94      0.94      0.94      3000\n",
            "\n",
            "[[262   2  22   2   2   0   0   2   0   3]\n",
            " [  6 272   5   0  12   3   0   1   0  12]\n",
            " [ 11   2 307   5   0   0   0   0   1   0]\n",
            " [  2   0  18 256   0   0   0   0  10   1]\n",
            " [  1   3   0   0 267   1   0   0   0   0]\n",
            " [  2   0   0   0   3 296   0   1   0   1]\n",
            " [  0   0   0   0   0   0 333   0   3   0]\n",
            " [  8   0   1   0   0   1   0 301   1   2]\n",
            " [  0   0   2   8   0   0   1   2 262   0]\n",
            " [  3   8   1   1   1   0   0   5   0 262]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "136613c2",
      "metadata": {
        "id": "136613c2"
      },
      "source": [
        "### Task 2.2: Tensorflow Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aae69f7",
      "metadata": {
        "id": "4aae69f7"
      },
      "source": [
        "In this part you will use the [Keras](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) to implement the [Neural Network](https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/) and apply it to the MNIST audio dataset (provided in part 1). Split the training dataset into train and evaluation data with 90:10 ratio. Run evaluation on X_eval while training on X_train. Tune the hyperparameters to get the best possible classification accuracy. You need to report accuracy, recall, precision and F1 score on the test dataset and print the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "278f1210",
      "metadata": {
        "id": "278f1210"
      },
      "outputs": [],
      "source": [
        "# Importing the modules\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset to train and test\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "\n",
        "print(\"x_train:\", x_train.shape)\n",
        "print(\"x_test:\", x_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRpkitjQ3ZKp",
        "outputId": "70a8e23a-43d4-464e-d59e-d8860eab4395"
      },
      "id": "VRpkitjQ3ZKp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train: (27000, 13)\n",
            "x_test: (3000, 13)\n",
            "y_train: (27000,)\n",
            "y_test (3000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting output to categorical \n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "EZAEVm8SNwyL"
      },
      "id": "EZAEVm8SNwyL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bce54023",
      "metadata": {
        "id": "bce54023",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78640240-669f-4580-d552-d868ca2b7806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.9852 - accuracy: 0.6596\n",
            "Epoch 2/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.3912 - accuracy: 0.8546\n",
            "Epoch 3/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.3312 - accuracy: 0.8780\n",
            "Epoch 4/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.2968 - accuracy: 0.8924\n",
            "Epoch 5/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.2776 - accuracy: 0.9001\n",
            "Epoch 6/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.2616 - accuracy: 0.9067\n",
            "Epoch 7/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.2483 - accuracy: 0.9109\n",
            "Epoch 8/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.2359 - accuracy: 0.9157\n",
            "Epoch 9/60\n",
            "270/270 [==============================] - 1s 3ms/step - loss: 0.2255 - accuracy: 0.9198\n",
            "Epoch 10/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.2174 - accuracy: 0.9221\n",
            "Epoch 11/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.2086 - accuracy: 0.9256\n",
            "Epoch 12/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1990 - accuracy: 0.9277\n",
            "Epoch 13/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1946 - accuracy: 0.9295\n",
            "Epoch 14/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9315\n",
            "Epoch 15/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1826 - accuracy: 0.9333\n",
            "Epoch 16/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.9364\n",
            "Epoch 17/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1725 - accuracy: 0.9378\n",
            "Epoch 18/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1665 - accuracy: 0.9408\n",
            "Epoch 19/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1648 - accuracy: 0.9402\n",
            "Epoch 20/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1616 - accuracy: 0.9418\n",
            "Epoch 21/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1564 - accuracy: 0.9431\n",
            "Epoch 22/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1528 - accuracy: 0.9450\n",
            "Epoch 23/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1521 - accuracy: 0.9455\n",
            "Epoch 24/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1468 - accuracy: 0.9473\n",
            "Epoch 25/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1442 - accuracy: 0.9493\n",
            "Epoch 26/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1394 - accuracy: 0.9495\n",
            "Epoch 27/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1373 - accuracy: 0.9515\n",
            "Epoch 28/60\n",
            "270/270 [==============================] - 1s 3ms/step - loss: 0.1370 - accuracy: 0.9510\n",
            "Epoch 29/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1358 - accuracy: 0.9513\n",
            "Epoch 30/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1315 - accuracy: 0.9522\n",
            "Epoch 31/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1300 - accuracy: 0.9536\n",
            "Epoch 32/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1286 - accuracy: 0.9528\n",
            "Epoch 33/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1240 - accuracy: 0.9549\n",
            "Epoch 34/60\n",
            "270/270 [==============================] - 1s 3ms/step - loss: 0.1234 - accuracy: 0.9541\n",
            "Epoch 35/60\n",
            "270/270 [==============================] - 1s 3ms/step - loss: 0.1238 - accuracy: 0.9557\n",
            "Epoch 36/60\n",
            "270/270 [==============================] - 1s 3ms/step - loss: 0.1190 - accuracy: 0.9568\n",
            "Epoch 37/60\n",
            "270/270 [==============================] - 1s 3ms/step - loss: 0.1180 - accuracy: 0.9580\n",
            "Epoch 38/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1151 - accuracy: 0.9585\n",
            "Epoch 39/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1131 - accuracy: 0.9583\n",
            "Epoch 40/60\n",
            "270/270 [==============================] - 1s 3ms/step - loss: 0.1123 - accuracy: 0.9601\n",
            "Epoch 41/60\n",
            "270/270 [==============================] - 1s 3ms/step - loss: 0.1107 - accuracy: 0.9605\n",
            "Epoch 42/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1086 - accuracy: 0.9610\n",
            "Epoch 43/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1085 - accuracy: 0.9599\n",
            "Epoch 44/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1103 - accuracy: 0.9596\n",
            "Epoch 45/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1051 - accuracy: 0.9626\n",
            "Epoch 46/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1046 - accuracy: 0.9612\n",
            "Epoch 47/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1031 - accuracy: 0.9627\n",
            "Epoch 48/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1012 - accuracy: 0.9633\n",
            "Epoch 49/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1007 - accuracy: 0.9631\n",
            "Epoch 50/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.0999 - accuracy: 0.9618\n",
            "Epoch 51/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.1014 - accuracy: 0.9623\n",
            "Epoch 52/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.0974 - accuracy: 0.9637\n",
            "Epoch 53/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.0984 - accuracy: 0.9635\n",
            "Epoch 54/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.0975 - accuracy: 0.9644\n",
            "Epoch 55/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.0959 - accuracy: 0.9648\n",
            "Epoch 56/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.0953 - accuracy: 0.9642\n",
            "Epoch 57/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.0929 - accuracy: 0.9668\n",
            "Epoch 58/60\n",
            "270/270 [==============================] - 1s 2ms/step - loss: 0.0930 - accuracy: 0.9659\n",
            "Epoch 59/60\n",
            "270/270 [==============================] - 1s 3ms/step - loss: 0.0928 - accuracy: 0.9653\n",
            "Epoch 60/60\n",
            "270/270 [==============================] - 1s 3ms/step - loss: 0.0893 - accuracy: 0.9674\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f175e728750>"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ],
      "source": [
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 60\n",
        "\n",
        "# Stacking the layers for model\n",
        "model = Sequential()\n",
        "model.add(Dense(60, input_shape=(13,), activation='relu'))\n",
        "model.add(Dense(40, input_shape=(60,), activation='relu'))\n",
        "model.add(Dense(15, input_shape=(40,), activation='relu'))\n",
        "model.add(Dense(10, activation='sigmoid'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE), metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting and measuring\n",
        "pred = model.predict(x_test)\n",
        "pred=np.argmax(pred, axis=1)\n",
        "y_test=np.argmax(y_test, axis=1)\n",
        "print(classification_report(y_test, pred))\n",
        "print(confusion_matrix(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA6NQDiLnBkg",
        "outputId": "49f25f25-92ee-4cc3-cfa8-5f669dd9a917"
      },
      "id": "gA6NQDiLnBkg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94/94 [==============================] - 0s 1ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       298\n",
            "           1       0.90      0.95      0.92       303\n",
            "           2       0.83      0.89      0.86       304\n",
            "           3       0.85      0.83      0.84       281\n",
            "           4       0.97      0.95      0.96       305\n",
            "           5       0.96      0.96      0.96       286\n",
            "           6       0.99      0.98      0.99       317\n",
            "           7       0.97      0.94      0.96       311\n",
            "           8       0.93      0.86      0.89       292\n",
            "           9       0.92      0.91      0.92       303\n",
            "\n",
            "    accuracy                           0.92      3000\n",
            "   macro avg       0.92      0.92      0.92      3000\n",
            "weighted avg       0.92      0.92      0.92      3000\n",
            "\n",
            "[[267   2  18   2   2   4   0   2   0   1]\n",
            " [  4 287   0   0   3   0   0   0   0   9]\n",
            " [ 29   2 270   0   1   0   0   0   0   2]\n",
            " [  5   0  28 233   0   1   0   1  12   1]\n",
            " [  5   7   0   0 289   3   0   0   0   1]\n",
            " [  1   2   0   0   2 274   0   2   1   4]\n",
            " [  0   0   1   0   0   0 312   0   4   0]\n",
            " [  6   1   5   0   0   1   0 293   0   5]\n",
            " [  1   0   1  37   0   0   2   0 250   1]\n",
            " [  0  18   2   1   0   1   0   3   1 277]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "611f8a4d",
      "metadata": {
        "id": "611f8a4d"
      },
      "source": [
        "### Task 2.3: Pytorch "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "457d441f",
      "metadata": {
        "id": "457d441f"
      },
      "source": [
        "In this part you will use the [Pytorch](https://pytorch.org/docs/stable/nn.html) to implement the [Neural Network](https://medium.com/analytics-vidhya/a-simple-neural-network-classifier-using-pytorch-from-scratch-7ebb477422d2) and apply it to the MNIST audio dataset (provided in part 1). Split the training dataset into train and evaluation data with 90:10 ratio. Run evaluation on X_eval while training on X_train. You need to use DataLoader to generate batches of data. Tune the hyperparameters to get the best possible classification accuracy. You need to report training loss, training accuracy, validation loss and validation accuracy after each epoch in the following format:\n",
        "```\n",
        "Epoch 1/2\n",
        "loss: 78.67749792151153 - accuracy: 0.6759259259259259 - val_loss: 6.320814955048263 - val_accuracy: 0.7356481481481482\n",
        "Epoch 2/2\n",
        "loss: 48.70551285566762 - accuracy: 0.7901234567901234 - val_loss: 6.073690168559551 - val_accuracy: 0.7791666666666667\n",
        "```\n",
        "You need to report accuracy, recall, precision and F1 score on the test dataset and print the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "604e1923",
      "metadata": {
        "id": "604e1923"
      },
      "outputs": [],
      "source": [
        "#importing the modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "\n",
        "x_train = x_train.to_numpy()\n",
        "x_test = x_test.to_numpy()\n",
        "y_train = y_train.to_numpy()\n",
        "y_test = y_test.to_numpy()\n",
        "\n",
        "print(\"x_train:\", x_train.shape)\n",
        "print(\"x_test:\", x_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPS08BzgWjx5",
        "outputId": "87f3dbe5-02ed-4425-b634-bfdfe34b4355"
      },
      "id": "mPS08BzgWjx5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train: (27000, 13)\n",
            "x_test: (3000, 13)\n",
            "y_train: (27000,)\n",
            "y_test (3000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdad8b6b",
      "metadata": {
        "id": "bdad8b6b"
      },
      "outputs": [],
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, X_train, y_train):\n",
        "      # Code here\n",
        "      self.X = torch.from_numpy(X_train.astype(np.float32))\n",
        "      self.y = torch.from_numpy(y_train).type(torch.LongTensor)\n",
        "      self.len = self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # Code here\n",
        "      return self.X[index], self.y[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "      # Code here\n",
        "      return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae160bf9",
      "metadata": {
        "id": "ae160bf9"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # Code here\n",
        "        self.linear1 = nn.Linear(13, 300)\n",
        "        self.linear2 = nn.Linear(300, 200)\n",
        "        self.linear3 = nn.Linear(200, 50)\n",
        "        self.linear6 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Code here\n",
        "        x = torch.sigmoid(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(testdata, testloader, model):\n",
        "  correct, total = 0, 0\n",
        "  running_loss = 0.0\n",
        "  # no need to calculate gradients during inference\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "      inputs, labels = data\n",
        "      # calculate output by running through the network\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      # get the predictions\n",
        "      __, predicted = torch.max(outputs.data, 1)\n",
        "      # update results\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      running_loss += loss.item()\n",
        "    \n",
        "  return correct // total, float(\"{0:.3f}\".format(running_loss/2000))\n"
      ],
      "metadata": {
        "id": "FA2lm49Ci_fL"
      },
      "id": "FA2lm49Ci_fL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c347a6c9",
      "metadata": {
        "id": "c347a6c9"
      },
      "outputs": [],
      "source": [
        "# Set the parameters accordingly\n",
        "LEARNING_RATE = 0.1\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576cacdf",
      "metadata": {
        "id": "576cacdf"
      },
      "outputs": [],
      "source": [
        "# Set the loss function and optimizer accordingly\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "# Initialize the model\n",
        "model = NeuralNetwork()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "traindata = Data(x_train, y_train)\n",
        "trainloader = DataLoader(traindata, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "testdata = Data(x_test, y_test)\n",
        "testloader = DataLoader(testdata, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "129965cc",
      "metadata": {
        "id": "129965cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400dd5e7-9221-4bc5-a851-b7fd554a1e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 \n",
            "loss: 0.063 - accuracy: 84 - val_loss: 0.007 - val_accuracy: 83\n",
            "\n",
            "Epoch 2/50 \n",
            "loss: 0.053 - accuracy: 86 - val_loss: 0.006 - val_accuracy: 85\n",
            "\n",
            "Epoch 3/50 \n",
            "loss: 0.047 - accuracy: 87 - val_loss: 0.005 - val_accuracy: 87\n",
            "\n",
            "Epoch 4/50 \n",
            "loss: 0.044 - accuracy: 88 - val_loss: 0.005 - val_accuracy: 88\n",
            "\n",
            "Epoch 5/50 \n",
            "loss: 0.04 - accuracy: 89 - val_loss: 0.005 - val_accuracy: 88\n",
            "\n",
            "Epoch 6/50 \n",
            "loss: 0.038 - accuracy: 90 - val_loss: 0.005 - val_accuracy: 88\n",
            "\n",
            "Epoch 7/50 \n",
            "loss: 0.036 - accuracy: 91 - val_loss: 0.004 - val_accuracy: 89\n",
            "\n",
            "Epoch 8/50 \n",
            "loss: 0.034 - accuracy: 91 - val_loss: 0.004 - val_accuracy: 90\n",
            "\n",
            "Epoch 9/50 \n",
            "loss: 0.032 - accuracy: 92 - val_loss: 0.004 - val_accuracy: 90\n",
            "\n",
            "Epoch 10/50 \n",
            "loss: 0.03 - accuracy: 92 - val_loss: 0.004 - val_accuracy: 91\n",
            "\n",
            "Epoch 11/50 \n",
            "loss: 0.03 - accuracy: 92 - val_loss: 0.004 - val_accuracy: 91\n",
            "\n",
            "Epoch 12/50 \n",
            "loss: 0.028 - accuracy: 93 - val_loss: 0.004 - val_accuracy: 91\n",
            "\n",
            "Epoch 13/50 \n",
            "loss: 0.027 - accuracy: 93 - val_loss: 0.003 - val_accuracy: 91\n",
            "\n",
            "Epoch 14/50 \n",
            "loss: 0.027 - accuracy: 93 - val_loss: 0.003 - val_accuracy: 91\n",
            "\n",
            "Epoch 15/50 \n",
            "loss: 0.025 - accuracy: 93 - val_loss: 0.003 - val_accuracy: 92\n",
            "\n",
            "Epoch 16/50 \n",
            "loss: 0.025 - accuracy: 94 - val_loss: 0.003 - val_accuracy: 92\n",
            "\n",
            "Epoch 17/50 \n",
            "loss: 0.024 - accuracy: 94 - val_loss: 0.003 - val_accuracy: 92\n",
            "\n",
            "Epoch 18/50 \n",
            "loss: 0.023 - accuracy: 94 - val_loss: 0.003 - val_accuracy: 92\n",
            "\n",
            "Epoch 19/50 \n",
            "loss: 0.022 - accuracy: 94 - val_loss: 0.003 - val_accuracy: 92\n",
            "\n",
            "Epoch 20/50 \n",
            "loss: 0.022 - accuracy: 95 - val_loss: 0.003 - val_accuracy: 93\n",
            "\n",
            "Epoch 21/50 \n",
            "loss: 0.021 - accuracy: 95 - val_loss: 0.003 - val_accuracy: 92\n",
            "\n",
            "Epoch 22/50 \n",
            "loss: 0.021 - accuracy: 94 - val_loss: 0.003 - val_accuracy: 93\n",
            "\n",
            "Epoch 23/50 \n",
            "loss: 0.02 - accuracy: 95 - val_loss: 0.003 - val_accuracy: 93\n",
            "\n",
            "Epoch 24/50 \n",
            "loss: 0.02 - accuracy: 95 - val_loss: 0.003 - val_accuracy: 93\n",
            "\n",
            "Epoch 25/50 \n",
            "loss: 0.019 - accuracy: 95 - val_loss: 0.003 - val_accuracy: 93\n",
            "\n",
            "Epoch 26/50 \n",
            "loss: 0.019 - accuracy: 95 - val_loss: 0.003 - val_accuracy: 93\n",
            "\n",
            "Epoch 27/50 \n",
            "loss: 0.019 - accuracy: 95 - val_loss: 0.003 - val_accuracy: 93\n",
            "\n",
            "Epoch 28/50 \n",
            "loss: 0.018 - accuracy: 96 - val_loss: 0.003 - val_accuracy: 93\n",
            "\n",
            "Epoch 29/50 \n",
            "loss: 0.018 - accuracy: 96 - val_loss: 0.003 - val_accuracy: 94\n",
            "\n",
            "Epoch 30/50 \n",
            "loss: 0.018 - accuracy: 96 - val_loss: 0.003 - val_accuracy: 93\n",
            "\n",
            "Epoch 31/50 \n",
            "loss: 0.017 - accuracy: 96 - val_loss: 0.003 - val_accuracy: 93\n",
            "\n",
            "Epoch 32/50 \n",
            "loss: 0.017 - accuracy: 96 - val_loss: 0.003 - val_accuracy: 94\n",
            "\n",
            "Epoch 33/50 \n",
            "loss: 0.017 - accuracy: 96 - val_loss: 0.003 - val_accuracy: 93\n",
            "\n",
            "Epoch 34/50 \n",
            "loss: 0.016 - accuracy: 96 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 35/50 \n",
            "loss: 0.016 - accuracy: 96 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 36/50 \n",
            "loss: 0.016 - accuracy: 96 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 37/50 \n",
            "loss: 0.015 - accuracy: 96 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 38/50 \n",
            "loss: 0.015 - accuracy: 96 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 39/50 \n",
            "loss: 0.015 - accuracy: 96 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 40/50 \n",
            "loss: 0.014 - accuracy: 96 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 41/50 \n",
            "loss: 0.014 - accuracy: 96 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 42/50 \n",
            "loss: 0.014 - accuracy: 97 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 43/50 \n",
            "loss: 0.014 - accuracy: 97 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 44/50 \n",
            "loss: 0.014 - accuracy: 96 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 45/50 \n",
            "loss: 0.013 - accuracy: 97 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 46/50 \n",
            "loss: 0.013 - accuracy: 97 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 47/50 \n",
            "loss: 0.013 - accuracy: 97 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 48/50 \n",
            "loss: 0.013 - accuracy: 97 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 49/50 \n",
            "loss: 0.013 - accuracy: 97 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n",
            "Epoch 50/50 \n",
            "loss: 0.012 - accuracy: 97 - val_loss: 0.002 - val_accuracy: 94\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = EPOCHS\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    inputs, labels = data\n",
        "    # set optimizer to zero grad to remove previous epoch gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward propagation\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # backward propagation\n",
        "    loss.backward()\n",
        "    # optimize\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  train_accuracy, train_loss = get_loss(traindata, trainloader, model)\n",
        "  test_accuracy, test_loss = get_loss(testdata, testloader, model)\n",
        "  print(f\"Epoch {epoch+1}/{epochs} \\nloss: {train_loss} - accuracy: {train_accuracy} - val_loss: {test_loss} - val_accuracy: {test_accuracy}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c1a74d8",
      "metadata": {
        "id": "6c1a74d8"
      },
      "source": [
        "## Part 3: Theoretical"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "055cffc2",
      "metadata": {
        "id": "055cffc2"
      },
      "source": [
        "**Q1**: Compare the tools you used above. State the advantages and disadvantages.\n",
        "\n",
        "**Ans**: If we compare the ease of implementation MLP is the easiest one then the Keras and in the last is Pytorch. Pytorch has relatively more complex implementation as compared to other.\n",
        "\n",
        "If we compare processing speed Pytroch is much faster.\n",
        "\n",
        "Debugging is much easier in Pytorch as compared to Keras.\n",
        "\n",
        "Keras is better for smaller data when you want to do fast implementation.\n",
        "\n",
        "**Q2**: What is the purpose of the data loader in PyTorch?\n",
        "\n",
        "**Ans**: Data Loader provides an iterable over the provided dataset. It provides two tensors the first one is the features and the second one is the output labels. The numbers of samples for each iteration depends upon the batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a3a8d4",
      "metadata": {
        "id": "e2a3a8d4"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}